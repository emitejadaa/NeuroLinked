{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a971eb0",
   "metadata": {},
   "source": [
    "# EEG Rest vs Left — Drive Loader (EDF) · Conv1d + Transformer · PyTorch Lightning\n",
    "*Generated: 2025-10-22 14:55:52*\n",
    "\n",
    "This notebook trains a **binary classifier (Rest=0 vs Left=1)** using your EDF files stored in:\n",
    "`content/drive/MyDrive/Colab-Dataset/eegmidb/organized_data`\n",
    "\n",
    "**Pipeline**\n",
    "1. Mount Drive & find EDFs under `sNNN/{rest,left}/*.edf` (right trials optional).\n",
    "2. Load with **MNE**, **band-pass** (1–40 Hz), optional **notch** (50/60 Hz), **resample** (e.g., 128 Hz).\n",
    "3. Slice into **sliding windows** (2 s, 50% overlap by default).\n",
    "4. Train **Conv1d → Transformer → MLP** (1 logit; BCEWithLogitsLoss).\n",
    "5. Evaluate & **export** model (.pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be799acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1) Environment & Drive\n",
    "!pip -q install mne==1.7.1 pytorch-lightning==2.4.0 torchmetrics==1.4.0\n",
    "\n",
    "import os, sys, math, json, random, glob, time, shutil, itertools\n",
    "import numpy as np\n",
    "import mne\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Torch:\", torch.__version__, \"| PL:\", pl.__version__, \"| MNE:\", mne.__version__)\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2) Config\n",
    "BASE_DIR = \"content/drive/MyDrive/Colab-Dataset/eegmidb/organized_data\"  #@param {type:\"string\"}\n",
    "USE_RIGHT_AS_NEGATIVE = False  #@param {type:\"boolean\"}\n",
    "BANDPASS_LOWER = 1.0  #@param {type:\"number\"}\n",
    "BANDPASS_UPPER = 40.0  #@param {type:\"number\"}\n",
    "APPLY_NOTCH = True     #@param {type:\"boolean\"}\n",
    "NOTCH_FREQ = 50.0      #@param {type:\"number\"}\n",
    "RESAMPLE_HZ = 128      #@param {type:\"number\"}\n",
    "WINDOW_SEC = 2.0       #@param {type:\"number\"}\n",
    "WINDOW_OVERLAP = 0.5   #@param {type:\"number\"}\n",
    "MAX_FILES_PER_CLASS = 0  # 0 = no limit  #@param {type:\"number\"}\n",
    "VAL_SPLIT = 0.15       #@param {type:\"number\"}\n",
    "TEST_SPLIT = 0.15      #@param {type:\"number\"}\n",
    "RANDOM_SEED = 42       #@param {type:\"number\"}\n",
    "BATCH_SIZE = 64        #@param {type:\"number\"}\n",
    "MAX_EPOCHS = 20        #@param {type:\"number\"}\n",
    "LR = 1e-3              #@param {type:\"number\"}\n",
    "DROPOUT = 0.1          #@param {type:\"number\"}\n",
    "\n",
    "seed_everything(RANDOM_SEED, workers=True)\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"; os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "LOGS_DIR = \"logs\"; os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "assert os.path.exists(BASE_DIR), f\"Path not found: {BASE_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1244c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3) Scan EDF files\n",
    "def list_edf_files(base_dir, subfolder):\n",
    "    pattern = os.path.join(base_dir, \"s*\", subfolder, \"*.edf\")\n",
    "    return sorted(glob.glob(pattern))\n",
    "\n",
    "left_files = list_edf_files(BASE_DIR, \"left\")\n",
    "rest_files = list_edf_files(BASE_DIR, \"rest\")\n",
    "right_files = list_edf_files(BASE_DIR, \"right\") if USE_RIGHT_AS_NEGATIVE else []\n",
    "\n",
    "if MAX_FILES_PER_CLASS > 0:\n",
    "    left_files = left_files[:MAX_FILES_PER_CLASS]\n",
    "    rest_files = rest_files[:MAX_FILES_PER_CLASS]\n",
    "    right_files = right_files[:MAX_FILES_PER_CLASS]\n",
    "\n",
    "print(f\"Found: left={len(left_files)}, rest={len(rest_files)}, right={len(right_files)}\")\n",
    "assert len(left_files) > 0 and len(rest_files) > 0, \"Need at least one EDF in both left and rest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec027d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4) EDF loader & preprocessing (MNE)\n",
    "def load_edf_preprocess(path, band=(BANDPASS_LOWER, BANDPASS_UPPER), notch=NOTCH_FREQ if APPLY_NOTCH else None, target_hz=RESAMPLE_HZ):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    eeg_picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=False, ecg=False, stim=False, misc=False)\n",
    "    if len(eeg_picks) == 0:\n",
    "        eeg_picks = list(range(len(raw.ch_names)))\n",
    "    raw.pick(eeg_picks)\n",
    "    raw.filter(band[0], band[1], fir_design='firwin', verbose=False)\n",
    "    if notch is not None and notch > 0:\n",
    "        raw.notch_filter(freqs=[notch], verbose=False)\n",
    "    if target_hz is not None:\n",
    "        raw.resample(target_hz, npad=\"auto\", verbose=False)\n",
    "    data = raw.get_data().astype(np.float32)  # (C, T)\n",
    "    return data, target_hz, raw.ch_names\n",
    "\n",
    "# sanity check\n",
    "_tmp = left_files[0]\n",
    "d, fs, ch = load_edf_preprocess(_tmp)\n",
    "print(\"Loaded:\", os.path.basename(_tmp), \"| shape:\", d.shape, \"| fs:\", fs, \"| EEG ch:\", len(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae888678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5) Sliding window epoching\n",
    "def make_windows(data, fs, window_sec=2.0, overlap=0.5):\n",
    "    C, T = data.shape\n",
    "    W = int(window_sec * fs)\n",
    "    step = max(1, int(W * (1 - overlap)))\n",
    "    out = []\n",
    "    for start in range(0, max(1, T - W + 1), step):\n",
    "        out.append(data[:, start:start+W])\n",
    "    return np.stack(out).astype(np.float32) if out else np.zeros((0, C, W), dtype=np.float32)\n",
    "\n",
    "w = make_windows(d, fs, WINDOW_SEC, WINDOW_OVERLAP)\n",
    "print(\"Windows shape:\", w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6) Dataset from Drive windows\n",
    "class EEGDriveDataset(Dataset):\n",
    "    def __init__(self, left_paths, rest_paths, right_paths=None):\n",
    "        self.items = []\n",
    "        for p in left_paths:\n",
    "            x, fs, _ = load_edf_preprocess(p)\n",
    "            for win in make_windows(x, fs, WINDOW_SEC, WINDOW_OVERLAP):\n",
    "                self.items.append((win, 1))\n",
    "        for p in rest_paths:\n",
    "            x, fs, _ = load_edf_preprocess(p)\n",
    "            for win in make_windows(x, fs, WINDOW_SEC, WINDOW_OVERLAP):\n",
    "                self.items.append((win, 0))\n",
    "        if right_paths:\n",
    "            for p in right_paths:\n",
    "                x, fs, _ = load_edf_preprocess(p)\n",
    "                for win in make_windows(x, fs, WINDOW_SEC, WINDOW_OVERLAP):\n",
    "                    self.items.append((win, 0))\n",
    "        random.shuffle(self.items)\n",
    "        if not self.items:\n",
    "            raise RuntimeError(\"Empty dataset.\")\n",
    "        self.C, self.W = self.items[0][0].shape\n",
    "        print(f\"Dataset: {len(self.items)} windows | C={self.C} | W={self.W}\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.items[idx]\n",
    "        return torch.from_numpy(x), torch.tensor([y], dtype=torch.float32)\n",
    "\n",
    "full_ds = EEGDriveDataset(left_files, rest_files, right_files if USE_RIGHT_AS_NEGATIVE else None)\n",
    "n_total = len(full_ds)\n",
    "n_test = int(TEST_SPLIT * n_total)\n",
    "n_val = int(VAL_SPLIT * (n_total - n_test))\n",
    "n_train = n_total - n_val - n_test\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "print(f\"Splits -> train:{len(train_ds)}  val:{len(val_ds)}  test:{len(test_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "EEG_CHANNELS = full_ds.C\n",
    "WINDOW_SAMPLES = full_ds.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7) Model: Conv1d → Transformer → MLP (1 logit)\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embed_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(x, x, x)\n",
    "        y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "        x = self.norm1(x + y)\n",
    "        y = self.mlp(x)\n",
    "        y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "        x = self.norm2(x + y)\n",
    "        return x\n",
    "\n",
    "class EEGClassificationModel(nn.Module):\n",
    "    def __init__(self, eeg_channels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(eeg_channels, eeg_channels, kernel_size=11, stride=1, padding=5, bias=False),\n",
    "            nn.BatchNorm1d(eeg_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(eeg_channels, eeg_channels * 2, kernel_size=11, stride=1, padding=5, bias=False),\n",
    "            nn.BatchNorm1d(eeg_channels * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        embed_dim = eeg_channels * 2\n",
    "        self.posenc = PositionalEncoding(embed_dim, dropout=dropout)\n",
    "        self.tr1 = TransformerBlock(embed_dim, num_heads=4, dim_feedforward=max(16, eeg_channels // 2), dropout=dropout)\n",
    "        self.tr2 = TransformerBlock(embed_dim, num_heads=4, dim_feedforward=max(16, eeg_channels // 2), dropout=dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, max(16, eeg_channels // 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(max(16, eeg_channels // 2), 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)        # (B, 2C, W)\n",
    "        x = x.permute(0, 2, 1)  # (B, W, 2C)\n",
    "        x = self.posenc(x)\n",
    "        x = self.tr1(x)\n",
    "        x = self.tr2(x)\n",
    "        x = x.mean(dim=1)       # (B, 2C)\n",
    "        x = self.mlp(x)         # (B, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc44e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8) LightningModule\n",
    "class LitEEG(pl.LightningModule):\n",
    "    def __init__(self, eeg_channels, lr=1e-3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = EEGClassificationModel(eeg_channels=eeg_channels, dropout=dropout)\n",
    "        self.lr = lr\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        sch = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[int(MAX_EPOCHS*0.5), int(MAX_EPOCHS*0.75)], gamma=0.1)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": sch}\n",
    "    def _common_step(self, batch, stage):\n",
    "        x, y = batch\n",
    "        logits = self(x).squeeze(1)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.squeeze(1))\n",
    "        preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "        acc = (preds == y.int().squeeze(1)).float().mean()\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.log(f\"{stage}_acc\", acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    def training_step(self, batch, batch_idx): return self._common_step(batch, \"train\")\n",
    "    def validation_step(self, batch, batch_idx): return self._common_step(batch, \"val\")\n",
    "    def test_step(self, batch, batch_idx): return self._common_step(batch, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d2078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9) Train\n",
    "lit_model = LitEEG(eeg_channels=EEG_CHANNELS, lr=LR, dropout=DROPOUT)\n",
    "logger_tb = TensorBoardLogger(save_dir=LOGS_DIR, name=\"tb\")\n",
    "logger_csv = CSVLogger(save_dir=LOGS_DIR, name=\"csv\")\n",
    "ckpt = ModelCheckpoint(dirpath=CHECKPOINT_DIR, monitor=\"val_acc\", mode=\"max\", save_top_k=1, filename=\"best\")\n",
    "lr_mon = LearningRateMonitor(logging_interval='epoch')\n",
    "es = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, min_delta=0.0)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if DEVICE==\"cuda\" else \"cpu\",\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    logger=[logger_tb, logger_csv],\n",
    "    callbacks=[ckpt, lr_mon, es],\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "print(\"Best checkpoint:\", ckpt.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10) Evaluate\n",
    "if ckpt.best_model_path and os.path.exists(ckpt.best_model_path):\n",
    "    model_best = LitEEG.load_from_checkpoint(ckpt.best_model_path, eeg_channels=EEG_CHANNELS, lr=LR, dropout=DROPOUT)\n",
    "else:\n",
    "    model_best = lit_model\n",
    "\n",
    "res = pl.Trainer(accelerator=\"gpu\" if DEVICE==\"cuda\" else \"cpu\", logger=False).test(model_best, dataloaders=test_loader)\n",
    "print(\"Test metrics:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cbf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 11) Inference demo\n",
    "model_best.eval().to(DEVICE)\n",
    "xb, yb = next(iter(test_loader))\n",
    "xb = xb.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits = model_best(xb).squeeze(1)\n",
    "    probs = torch.sigmoid(logits)\n",
    "preds = (probs > 0.5).int().cpu().numpy()\n",
    "print(\"First 16 preds (0=rest,1=left):\", preds[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 12) Export model (.pt)\n",
    "EXPORT_DIR = \"exports\"; os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "export_path = os.path.join(EXPORT_DIR, \"eeg_rest_vs_left.pt\")\n",
    "scripted = torch.jit.script(model_best.model.cpu())\n",
    "scripted.save(export_path)\n",
    "print(\"Saved:\", export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e934e",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- To include **right** trials as negative samples, set `USE_RIGHT_AS_NEGATIVE=True`.\n",
    "- Consider balancing classes or weighted loss if classes are imbalanced.\n",
    "- You can tweak `WINDOW_SEC`, `WINDOW_OVERLAP`, and `RESAMPLE_HZ` to match your prior setup."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
