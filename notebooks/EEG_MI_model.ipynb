{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0999ba",
   "metadata": {},
   "source": [
    "# EEG Rest vs Left (Drive · EDF) — Conv1d + Transformer + Lightning\n",
    "*Generated: 2025-10-22 15:18:31*\n",
    "\n",
    "Este notebook reproduce la lógica del original (clases **rest=0** vs **left=1**), pero cargando desde tu estructura:\n",
    "```\n",
    "/content/drive/MyDrive/Colab-Dataset/eegmidb/organized_data/sNNN/{rest,right,left}/*.edf\n",
    "```\n",
    "- Escaneo **recursivo**, **case-insensitive** y acepta **.edf/.EDF**.\n",
    "- Etiqueta por el **nombre de carpeta** (no depende del nombre del archivo).\n",
    "- Dataset tolerante: **salta** archivos problemáticos y te los lista.\n",
    "- Modelo: **Conv1d → Transformer → MLP (1 logit)** · BCEWithLogitsLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1) Entorno & Montar Drive\n",
    "!pip -q install mne==1.7.1 pytorch-lightning==2.4.0\n",
    "\n",
    "import os, sys, math, random, glob\n",
    "import numpy as np\n",
    "import mne\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Torch:\", torch.__version__, \"| PL:\", pl.__version__, \"| MNE:\", mne.__version__)\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb329de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2) Configuración\n",
    "BASE_DIR = \"/content/drive/MyDrive/Colab-Dataset/eegmidb/organized_data\"  #@param {type:\"string\"}\n",
    "USE_RIGHT_AS_NEGATIVE = False  #@param {type:\"boolean\"}\n",
    "INCLUDE_PATTERNS = [\"*.edf\", \"*.EDF\"]\n",
    "\n",
    "LEFT_DIRNAMES = [\"left\"]   #@param {type:\"raw\"}\n",
    "REST_DIRNAMES = [\"rest\"]   #@param {type:\"raw\"}\n",
    "RIGHT_DIRNAMES = [\"right\"] #@param {type:\"raw\"}\n",
    "\n",
    "BANDPASS_LOWER = 1.0  #@param {type:\"number\"}\n",
    "BANDPASS_UPPER = 40.0 #@param {type:\"number\"}\n",
    "APPLY_NOTCH = True    #@param {type:\"boolean\"}\n",
    "NOTCH_FREQ = 50.0     #@param {type:\"number\"}\n",
    "RESAMPLE_HZ = 128     #@param {type:\"number\"}\n",
    "\n",
    "WINDOW_SEC = 2.0      #@param {type:\"number\"}\n",
    "WINDOW_OVERLAP = 0.5  #@param {type:\"number\"}\n",
    "\n",
    "MAX_FILES_PER_CLASS = 0   # 0 = sin límite  #@param {type:\"number\"}\n",
    "VAL_SPLIT = 0.15          #@param {type:\"number\"}\n",
    "TEST_SPLIT = 0.15         #@param {type:\"number\"}\n",
    "RANDOM_SEED = 42          #@param {type:\"number\"}\n",
    "BATCH_SIZE = 64           #@param {type:\"number\"}\n",
    "MAX_EPOCHS = 20           #@param {type:\"number\"}\n",
    "LR = 1e-3                 #@param {type:\"number\"}\n",
    "DROPOUT = 0.1             #@param {type:\"number\"}\n",
    "\n",
    "seed_everything(RANDOM_SEED, workers=True)\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"; os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "LOGS_DIR = \"logs\"; os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "assert os.path.exists(BASE_DIR), f\"Path not found: {BASE_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3) Debug rápido de la ruta base\n",
    "print(\"BASE_DIR =\", BASE_DIR)\n",
    "print(\"¿Existe? ->\", os.path.exists(BASE_DIR))\n",
    "print(\"Ejemplo de subdirectorios dentro de BASE_DIR:\")\n",
    "for p in sorted([os.path.join(BASE_DIR, x) for x in os.listdir(BASE_DIR)])[:10]:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4) Escaneo recursivo robusto (case-insensitive)\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "def scan_edf_paths(base_dir, include_patterns):\n",
    "    paths = []\n",
    "    for root, _, filenames in os.walk(base_dir):\n",
    "        for fn in filenames:\n",
    "            for pat in include_patterns:\n",
    "                if fnmatch(fn, pat):\n",
    "                    full = os.path.join(root, fn)\n",
    "                    if os.path.isfile(full):\n",
    "                        paths.append(full)\n",
    "                    break\n",
    "    return sorted(paths)\n",
    "\n",
    "def classify_by_parent_folder(path, left_names, rest_names, right_names):\n",
    "    parts = [p.casefold() for p in os.path.normpath(path).split(os.sep)]\n",
    "    if any(seg in {n.casefold() for n in left_names} for seg in parts):  return \"left\"\n",
    "    if any(seg in {n.casefold() for n in rest_names} for seg in parts):  return \"rest\"\n",
    "    if any(seg in {n.casefold() for n in right_names} for seg in parts): return \"right\"\n",
    "    return None\n",
    "\n",
    "all_edfs = scan_edf_paths(BASE_DIR, INCLUDE_PATTERNS)\n",
    "left_files, rest_files, right_files = [], [], []\n",
    "\n",
    "for p in all_edfs:\n",
    "    cls = classify_by_parent_folder(p, LEFT_DIRNAMES, REST_DIRNAMES, RIGHT_DIRNAMES)\n",
    "    if cls == \"left\": left_files.append(p)\n",
    "    elif cls == \"rest\": rest_files.append(p)\n",
    "    elif cls == \"right\" and USE_RIGHT_AS_NEGATIVE: right_files.append(p)\n",
    "\n",
    "if MAX_FILES_PER_CLASS > 0:\n",
    "    left_files  = left_files[:MAX_FILES_PER_CLASS]\n",
    "    rest_files  = rest_files[:MAX_FILES_PER_CLASS]\n",
    "    right_files = right_files[:MAX_FILES_PER_CLASS]\n",
    "\n",
    "left_files  = [p for p in left_files  if os.path.exists(p)]\n",
    "rest_files  = [p for p in rest_files  if os.path.exists(p)]\n",
    "right_files = [p for p in right_files if os.path.exists(p)]\n",
    "\n",
    "print(f\"EDFs totales escaneados: {len(all_edfs)}\")\n",
    "print(f\"→ left={len(left_files)} | rest={len(rest_files)} | right(neg)={len(right_files)}\")\n",
    "print(\"Sample left:\", left_files[:3])\n",
    "print(\"Sample rest:\", rest_files[:3])\n",
    "\n",
    "assert len(left_files) > 0 and len(rest_files) > 0, \"Need at least one EDF in both left and rest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5) Loader EDF + Preprocesado (MNE)\n",
    "def load_edf_preprocess(path, band=(BANDPASS_LOWER, BANDPASS_UPPER), notch=NOTCH_FREQ if APPLY_NOTCH else None, target_hz=RESAMPLE_HZ):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    eeg_picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=False, ecg=False, stim=False, misc=False)\n",
    "    if len(eeg_picks) == 0:\n",
    "        eeg_picks = list(range(len(raw.ch_names)))\n",
    "    raw.pick(eeg_picks)\n",
    "    raw.filter(band[0], band[1], fir_design='firwin', verbose=False)\n",
    "    if notch is not None and notch > 0:\n",
    "        raw.notch_filter(freqs=[notch], verbose=False)\n",
    "    if target_hz is not None:\n",
    "        raw.resample(target_hz, npad=\"auto\", verbose=False)\n",
    "    data = raw.get_data().astype(np.float32)  # (C, T)\n",
    "    return data, target_hz, raw.ch_names\n",
    "\n",
    "d, fs, ch = load_edf_preprocess(left_files[0])\n",
    "print(\"Ejemplo cargado:\", os.path.basename(left_files[0]), \"| shape:\", d.shape, \"| fs:\", fs, \"| EEG ch:\", len(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6) Ventaneo deslizante\n",
    "def make_windows(data, fs, window_sec=2.0, overlap=0.5):\n",
    "    C, T = data.shape\n",
    "    W = int(window_sec * fs)\n",
    "    step = max(1, int(W * (1 - overlap)))\n",
    "    out = []\n",
    "    for start in range(0, max(1, T - W + 1), step):\n",
    "        out.append(data[:, start:start+W])\n",
    "    return np.stack(out).astype(np.float32) if out else np.zeros((0, C, W), dtype=np.float32)\n",
    "\n",
    "print(\"Windows ejemplo:\", make_windows(d, fs, WINDOW_SEC, WINDOW_OVERLAP).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bbfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7) Dataset tolerante a errores\n",
    "class EEGDriveDataset(Dataset):\n",
    "    def __init__(self, left_paths, rest_paths, right_paths=None):\n",
    "        self.items = []\n",
    "        self.bad_files = []\n",
    "\n",
    "        def add_class(paths, label):\n",
    "            for p in paths:\n",
    "                try:\n",
    "                    x, fs, _ = load_edf_preprocess(p)\n",
    "                    wins = make_windows(x, fs, WINDOW_SEC, WINDOW_OVERLAP)\n",
    "                    for w in wins:\n",
    "                        self.items.append((w, label))\n",
    "                except FileNotFoundError:\n",
    "                    self.bad_files.append((p, \"FileNotFoundError\"))\n",
    "                except Exception as e:\n",
    "                    self.bad_files.append((p, f\"{type(e).__name__}: {e}\"))\n",
    "\n",
    "        add_class(left_paths, 1)\n",
    "        add_class(rest_paths, 0)\n",
    "        if right_paths:\n",
    "            add_class(right_paths, 0)\n",
    "\n",
    "        random.shuffle(self.items)\n",
    "        if not self.items:\n",
    "            raise RuntimeError(\"Empty dataset (todos los archivos fallaron o no hay ventanas).\")\n",
    "\n",
    "        self.C, self.W = self.items[0][0].shape\n",
    "        print(f\"Dataset: {len(self.items)} windows | C={self.C} | W={self.W}\")\n",
    "        if self.bad_files:\n",
    "            print(f\"Avisos: {len(self.bad_files)} archivos salteados por error. Ejemplos:\")\n",
    "            for p, err in self.bad_files[:5]:\n",
    "                print(\"  -\", p, \"=>\", err)\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.items[idx]\n",
    "        return torch.from_numpy(x), torch.tensor([y], dtype=torch.float32)\n",
    "\n",
    "full_ds = EEGDriveDataset(left_files, rest_files, right_files if USE_RIGHT_AS_NEGATIVE else None)\n",
    "n_total = len(full_ds)\n",
    "n_test = int(TEST_SPLIT * n_total)\n",
    "n_val  = int(VAL_SPLIT * (n_total - n_test))\n",
    "n_train = n_total - n_val - n_test\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "print(f\"Splits -> train:{len(train_ds)}  val:{len(val_ds)}  test:{len(test_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "EEG_CHANNELS = full_ds.C\n",
    "WINDOW_SAMPLES = full_ds.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8) Modelo: Conv1d → Transformer → MLP (1 logit)\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embed_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(x, x, x)\n",
    "        y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "        x = self.norm1(x + y)\n",
    "        y = self.mlp(x)\n",
    "        y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "        x = self.norm2(x + y)\n",
    "        return x\n",
    "\n",
    "class EEGClassificationModel(nn.Module):\n",
    "    def __init__(self, eeg_channels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(eeg_channels, eeg_channels, kernel_size=11, stride=1, padding=5, bias=False),\n",
    "            nn.BatchNorm1d(eeg_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(eeg_channels, eeg_channels * 2, kernel_size=11, stride=1, padding=5, bias=False),\n",
    "            nn.BatchNorm1d(eeg_channels * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        embed_dim = eeg_channels * 2\n",
    "        self.posenc = PositionalEncoding(embed_dim, dropout=dropout)\n",
    "        self.tr1 = TransformerBlock(embed_dim, num_heads=4, dim_feedforward=max(16, eeg_channels // 2), dropout=dropout)\n",
    "        self.tr2 = TransformerBlock(embed_dim, num_heads=4, dim_feedforward=max(16, eeg_channels // 2), dropout=dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, max(16, eeg_channels // 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(max(16, eeg_channels // 2), 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.posenc(x)\n",
    "        x = self.tr1(x); x = self.tr2(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9) LightningModule\n",
    "class LitEEG(pl.LightningModule):\n",
    "    def __init__(self, eeg_channels, lr=1e-3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = EEGClassificationModel(eeg_channels=eeg_channels, dropout=dropout)\n",
    "        self.lr = lr\n",
    "    def forward(self, x): return self.model(x)\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        sch = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[int(MAX_EPOCHS*0.5), int(MAX_EPOCHS*0.75)], gamma=0.1)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": sch}\n",
    "    def _step(self, batch, stage):\n",
    "        x, y = batch\n",
    "        logits = self(x).squeeze(1)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.squeeze(1))\n",
    "        preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "        acc = (preds == y.int().squeeze(1)).float().mean()\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.log(f\"{stage}_acc\", acc,  prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    def training_step(self, b, i): return self._step(b, \"train\")\n",
    "    def validation_step(self, b, i): return self._step(b, \"val\")\n",
    "    def test_step(self, b, i): return self._step(b, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10) Entrenar\n",
    "lit_model = LitEEG(eeg_channels=EEG_CHANNELS, lr=LR, dropout=DROPOUT)\n",
    "\n",
    "logger_tb = TensorBoardLogger(save_dir=LOGS_DIR, name=\"tb\")\n",
    "logger_csv = CSVLogger(save_dir=LOGS_DIR, name=\"csv\")\n",
    "ckpt   = ModelCheckpoint(dirpath=CHECKPOINT_DIR, monitor=\"val_acc\", mode=\"max\", save_top_k=1, filename=\"best\")\n",
    "lr_mon = LearningRateMonitor(logging_interval='epoch')\n",
    "es     = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, min_delta=0.0)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if DEVICE==\"cuda\" else \"cpu\",\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    logger=[logger_tb, logger_csv],\n",
    "    callbacks=[ckpt, lr_mon, es],\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "print(\"Best checkpoint:\", ckpt.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02199dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 11) Evaluación + Inference demo\n",
    "best = LitEEG.load_from_checkpoint(ckpt.best_model_path, eeg_channels=EEG_CHANNELS, lr=LR, dropout=DROPOUT) if ckpt.best_model_path else lit_model\n",
    "res = pl.Trainer(accelerator=\"gpu\" if DEVICE==\"cuda\" else \"cpu\", logger=False).test(best, dataloaders=test_loader)\n",
    "print(\"Test metrics:\", res)\n",
    "\n",
    "best.eval().to(DEVICE)\n",
    "xb, yb = next(iter(test_loader))\n",
    "xb = xb.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits = best(xb).squeeze(1)\n",
    "    probs = torch.sigmoid(logits)\n",
    "preds = (probs > 0.5).int().cpu().numpy()\n",
    "print(\"Preds (0=rest,1=left) primeros 16:\", preds[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 12) Exportar modelo (.pt)\n",
    "EXPORT_DIR = \"exports\"; os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "export_path = os.path.join(EXPORT_DIR, \"eeg_rest_vs_left.pt\")\n",
    "scripted = torch.jit.script(best.model.cpu())\n",
    "scripted.save(export_path)\n",
    "print(\"Saved:\", export_path)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
