{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00a0a0c",
   "metadata": {},
   "source": [
    "# EEG Rest vs Left — Drive Loader (EDF) · Conv1d + Transformer · PyTorch Lightning\n",
    "*Generated: 2025-10-22 15:06:44*\n",
    "\n",
    "Version **fix**: robust scanner (case-insensitive, recursive), corrected imports, absolute Drive path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1) Environment & Drive\n",
    "!pip -q install mne==1.7.1 pytorch-lightning==2.4.0 torchmetrics==1.4.0\n",
    "\n",
    "import os, sys, math, json, random, glob, time, shutil, itertools\n",
    "import numpy as np\n",
    "import mne\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Torch:\", torch.__version__, \"| PL:\", pl.__version__, \"| MNE:\", mne.__version__)\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2) Config\n",
    "BASE_DIR = \"/content/drive/MyDrive/Colab-Dataset/eegmidb/organized_data\"  #@param {type:\"string\"}\n",
    "USE_RIGHT_AS_NEGATIVE = False  #@param {type:\"boolean\"}\n",
    "INCLUDE_PATTERNS = [\"*.edf\", \"*.EDF\"]  # accepted extensions\n",
    "LEFT_DIRNAMES = [\"left\"]   #@param {type:\"raw\"}\n",
    "REST_DIRNAMES = [\"rest\",\"baseline\",\"idle\"]  #@param {type:\"raw\"}\n",
    "RIGHT_DIRNAMES = [\"right\"]  # used only if USE_RIGHT_AS_NEGATIVE\n",
    "BANDPASS_LOWER = 1.0  #@param {type:\"number\"}\n",
    "BANDPASS_UPPER = 40.0  #@param {type:\"number\"}\n",
    "APPLY_NOTCH = True     #@param {type:\"boolean\"}\n",
    "NOTCH_FREQ = 50.0      #@param {type:\"number\"}\n",
    "RESAMPLE_HZ = 128      #@param {type:\"number\"}\n",
    "WINDOW_SEC = 2.0       #@param {type:\"number\"}\n",
    "WINDOW_OVERLAP = 0.5   #@param {type:\"number\"}\n",
    "MAX_FILES_PER_CLASS = 0  # 0 = no limit  #@param {type:\"number\"}\n",
    "VAL_SPLIT = 0.15       #@param {type:\"number\"}\n",
    "TEST_SPLIT = 0.15      #@param {type:\"number\"}\n",
    "RANDOM_SEED = 42       #@param {type:\"number\"}\n",
    "BATCH_SIZE = 64        #@param {type:\"number\"}\n",
    "MAX_EPOCHS = 20        #@param {type:\"number\"}\n",
    "LR = 1e-3              #@param {type:\"number\"}\n",
    "DROPOUT = 0.1          #@param {type:\"number\"}\n",
    "\n",
    "seed_everything(RANDOM_SEED, workers=True)\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"; os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "LOGS_DIR = \"logs\"; os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "assert os.path.exists(BASE_DIR), f\"Path not found: {BASE_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3) Robust recursive scan (case-insensitive by folder name)\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "def scan_edf_paths(base_dir, include_patterns):\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(base_dir):\n",
    "        for fn in filenames:\n",
    "            for pat in include_patterns:\n",
    "                if fnmatch(fn, pat):\n",
    "                    files.append(os.path.join(root, fn))\n",
    "                    break\n",
    "    return sorted(files)\n",
    "\n",
    "def classify_by_parent_folder(path, left_names, rest_names, right_names):\n",
    "    parts = [p.casefold() for p in os.path.normpath(path).split(os.sep)]\n",
    "    # check any segment equals a known label folder\n",
    "    if any(seg in set(n.casefold() for n in left_names) for seg in parts):\n",
    "        return \"left\"\n",
    "    if any(seg in set(n.casefold() for n in rest_names) for seg in parts):\n",
    "        return \"rest\"\n",
    "    if any(seg in set(n.casefold() for n in right_names) for seg in parts):\n",
    "        return \"right\"\n",
    "    return None\n",
    "\n",
    "all_edfs = scan_edf_paths(BASE_DIR, INCLUDE_PATTERNS)\n",
    "left_files, rest_files, right_files = [], [], []\n",
    "\n",
    "for p in all_edfs:\n",
    "    cls = classify_by_parent_folder(p, LEFT_DIRNAMES, REST_DIRNAMES, RIGHT_DIRNAMES)\n",
    "    if cls == \"left\":\n",
    "        left_files.append(p)\n",
    "    elif cls == \"rest\":\n",
    "        rest_files.append(p)\n",
    "    elif cls == \"right\" and USE_RIGHT_AS_NEGATIVE:\n",
    "        right_files.append(p)\n",
    "\n",
    "if MAX_FILES_PER_CLASS > 0:\n",
    "    left_files = left_files[:MAX_FILES_PER_CLASS]\n",
    "    rest_files = rest_files[:MAX_FILES_PER_CLASS]\n",
    "    right_files = right_files[:MAX_FILES_PER_CLASS]\n",
    "\n",
    "print(f\"Found EDFs total: {len(all_edfs)}\")\n",
    "print(f\"→ left={len(left_files)} | rest={len(rest_files)} | right(neg)={len(right_files)}\")\n",
    "print(\"Sample left:\", left_files[:3])\n",
    "print(\"Sample rest:\", rest_files[:3])\n",
    "\n",
    "assert len(left_files) > 0 and len(rest_files) > 0, \"Need at least one EDF in both left and rest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4) EDF loader & preprocessing (MNE)\n",
    "def load_edf_preprocess(path, band=(BANDPASS_LOWER, BANDPASS_UPPER), notch=NOTCH_FREQ if APPLY_NOTCH else None, target_hz=RESAMPLE_HZ):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    eeg_picks = mne.pick_types(raw.info, meg=False, eeg=True, eog=False, ecg=False, stim=False, misc=False)\n",
    "    if len(eeg_picks) == 0:\n",
    "        eeg_picks = list(range(len(raw.ch_names)))\n",
    "    raw.pick(eeg_picks)\n",
    "    raw.filter(band[0], band[1], fir_design='firwin', verbose=False)\n",
    "    if notch is not None and notch > 0:\n",
    "        raw.notch_filter(freqs=[notch], verbose=False)\n",
    "    if target_hz is not None:\n",
    "        raw.resample(target_hz, npad=\"auto\", verbose=False)\n",
    "    data = raw.get_data().astype(np.float32)  # (C, T)\n",
    "    return data, target_hz, raw.ch_names\n",
    "\n",
    "# quick check\n",
    "d, fs, ch = load_edf_preprocess(left_files[0])\n",
    "print(\"Loaded example:\", os.path.basename(left_files[0]), \"| shape:\", d.shape, \"| fs:\", fs, \"| EEG ch:\", len(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7913e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5) Sliding window epoching\n",
    "def make_windows(data, fs, window_sec=2.0, overlap=0.5):\n",
    "    C, T = data.shape\n",
    "    W = int(window_sec * fs)\n",
    "    step = max(1, int(W * (1 - overlap)))\n",
    "    out = []\n",
    "    for start in range(0, max(1, T - W + 1), step):\n",
    "        out.append(data[:, start:start+W])\n",
    "    return np.stack(out).astype(np.float32) if out else np.zeros((0, C, W), dtype=np.float32)\n",
    "\n",
    "print(\"Windows shape example:\", make_windows(d, fs, WINDOW_SEC, WINDOW_OVERLAP).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfbd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6) Dataset\n",
    "class EEGDriveDataset(Dataset):\n",
    "    def __init__(self, left_paths, rest_paths, right_paths=None):\n",
    "        self.items = []\n",
    "        for p in left_paths:\n",
    "            x, fs, _ = load_edf_preprocess(p)\n",
    "            for win in make_windows(x, fs, WINDOW_SEC, WINDOW_OVERLAP):\n",
    "                self.items.append((win, 1))\n",
    "        for p in rest_paths:\n",
    "            x, fs, _ = load_edf_preprocess(p)\n",
    "            for win in make_windows(x, fs, WINDOW_SEC, WINDOW_OVERLAP):\n",
    "                self.items.append((win, 0))\n",
    "        if right_paths:\n",
    "            for p in right_paths:\n",
    "                x, fs, _ = load_edf_preprocess(p)\n",
    "                for win in make_windows(x, fs, WINDOW_SEC, WINDOW_OVERLAP):\n",
    "                    self.items.append((win, 0))\n",
    "        random.shuffle(self.items)\n",
    "        if not self.items:\n",
    "            raise RuntimeError(\"Empty dataset.\")\n",
    "        self.C, self.W = self.items[0][0].shape\n",
    "        print(f\"Dataset: {len(self.items)} windows | C={self.C} | W={self.W}\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.items[idx]\n",
    "        return torch.from_numpy(x), torch.tensor([y], dtype=torch.float32)\n",
    "\n",
    "full_ds = EEGDriveDataset(left_files, rest_files, right_files if USE_RIGHT_AS_NEGATIVE else None)\n",
    "n_total = len(full_ds)\n",
    "n_test = int(TEST_SPLIT * n_total)\n",
    "n_val = int(VAL_SPLIT * (n_total - n_test))\n",
    "n_train = n_total - n_val - n_test\n",
    "from torch.utils.data import random_split\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "print(f\"Splits -> train:{len(train_ds)}  val:{len(val_ds)}  test:{len(test_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "EEG_CHANNELS = full_ds.C\n",
    "WINDOW_SAMPLES = full_ds.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7) Model\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embed_dim),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(x, x, x)\n",
    "        y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "        x = self.norm1(x + y)\n",
    "        y = self.mlp(x)\n",
    "        y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "        x = self.norm2(x + y)\n",
    "        return x\n",
    "\n",
    "class EEGClassificationModel(nn.Module):\n",
    "    def __init__(self, eeg_channels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(eeg_channels, eeg_channels, kernel_size=11, stride=1, padding=5, bias=False),\n",
    "            nn.BatchNorm1d(eeg_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(eeg_channels, eeg_channels * 2, kernel_size=11, stride=1, padding=5, bias=False),\n",
    "            nn.BatchNorm1d(eeg_channels * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        embed_dim = eeg_channels * 2\n",
    "        self.posenc = PositionalEncoding(embed_dim, dropout=dropout)\n",
    "        self.tr1 = TransformerBlock(embed_dim, num_heads=4, dim_feedforward=max(16, eeg_channels // 2), dropout=dropout)\n",
    "        self.tr2 = TransformerBlock(embed_dim, num_heads=4, dim_feedforward=max(16, eeg_channels // 2), dropout=dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, max(16, eeg_channels // 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(max(16, eeg_channels // 2), 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.posenc(x)\n",
    "        x = self.tr1(x); x = self.tr2(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8) Lightning wrapper\n",
    "class LitEEG(pl.LightningModule):\n",
    "    def __init__(self, eeg_channels, lr=1e-3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = EEGClassificationModel(eeg_channels=eeg_channels, dropout=dropout)\n",
    "        self.lr = lr\n",
    "    def forward(self, x): return self.model(x)\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "        sch = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[int(MAX_EPOCHS*0.5), int(MAX_EPOCHS*0.75)], gamma=0.1)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": sch}\n",
    "    def _common(self, batch, stage):\n",
    "        x, y = batch\n",
    "        logits = self(x).squeeze(1)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.squeeze(1))\n",
    "        preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "        acc = (preds == y.int().squeeze(1)).float().mean()\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.log(f\"{stage}_acc\", acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    def training_step(self, b, i): return self._common(b, \"train\")\n",
    "    def validation_step(self, b, i): return self._common(b, \"val\")\n",
    "    def test_step(self, b, i): return self._common(b, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cea48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9) Train\n",
    "lit_model = LitEEG(eeg_channels=EEG_CHANNELS, lr=LR, dropout=DROPOUT)\n",
    "logger_tb = TensorBoardLogger(save_dir=LOGS_DIR, name=\"tb\")\n",
    "logger_csv = CSVLogger(save_dir=LOGS_DIR, name=\"csv\")\n",
    "ckpt = ModelCheckpoint(dirpath=CHECKPOINT_DIR, monitor=\"val_acc\", mode=\"max\", save_top_k=1, filename=\"best\")\n",
    "lr_mon = LearningRateMonitor(logging_interval='epoch')\n",
    "es = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, min_delta=0.0)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if DEVICE==\"cuda\" else \"cpu\",\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    logger=[logger_tb, logger_csv],\n",
    "    callbacks=[ckpt, lr_mon, es],\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "print(\"Best checkpoint:\", ckpt.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10) Test\n",
    "best = LitEEG.load_from_checkpoint(ckpt.best_model_path, eeg_channels=EEG_CHANNELS, lr=LR, dropout=DROPOUT) if ckpt.best_model_path else lit_model\n",
    "res = pl.Trainer(accelerator=\"gpu\" if DEVICE==\"cuda\" else \"cpu\", logger=False).test(best, dataloaders=test_loader)\n",
    "print(\"Test metrics:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24046894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 11) Export\n",
    "EXPORT_DIR = \"exports\"; os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "export_path = os.path.join(EXPORT_DIR, \"eeg_rest_vs_left.pt\")\n",
    "scripted = torch.jit.script(best.model.cpu())\n",
    "scripted.save(export_path)\n",
    "print(\"Saved:\", export_path)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
